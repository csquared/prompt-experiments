{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79afe886-b188-4843-9e26-cff974534a35",
   "metadata": {},
   "source": [
    "# Prompt Experiments: impact of pronouns\n",
    "When constructing a system prompt, is it better to operate from 1st, 2nd, or 2nd person plural?\n",
    "We will construct some prompts and see if there is any major difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6a7385-d75f-439f-97d9-5d6b35a8ad8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b369d39c-79d2-45b5-a8f4-c6f392c4ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2526ac96-b480-4c1b-8558-b5cf38760f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prompt = \"\"\"\n",
    "Given the following function:\n",
    "```python\n",
    "def calculate_area_rectangular_floor(width, length):\n",
    "  return width*length\n",
    "```\n",
    "How would you improve it?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e18201c1-87db-4d21-be94-b0ad4f026ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_1st_person = \"\"\"\n",
    "I am a software engineer with 20 years of experience in python and mentoring in coding.\n",
    "When presented with code snippets, I suggest how they can be improved by adding comments, \n",
    "improving performance, enhacing readibility, or whatever else I can think of.\n",
    "I explain my reasoning clearly and concisely.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0608e82-b1de-47fc-ac23-0af857b1a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_2nd_person = \"\"\"\n",
    "You are a software engineer with 20 years of experience in python and mentoring in coding.\n",
    "When presented with code snippets, you suggest how they can be improved by adding comments, \n",
    "improving performance, enhacing readibility, or whatever else you can think of.\n",
    "You explain your reasoning clearly and concisely.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef2369c5-eb22-4d2d-89f6-7b247c1758c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_royal_we = \"\"\"\n",
    "We are a software engineer with 20 years of experience in python and mentoring in coding.\n",
    "When presented with code snippets, we suggest how they can be improved by adding comments, \n",
    "improving performance, enhacing readibility, or whatever else you can think of.\n",
    "We explain our reasoning clearly and concisely.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "193b2071-c775-4c7c-85ef-e13c3fe16e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(system_prompt, user_prompt=common_prompt):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.0,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56b86ef4-f347-4828-b7cc-b05b8de50288",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_1st_response = make_request(system_1st_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd9475de-53ff-47f8-b8c6-99ff4c571159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few ways to improve the given function:\n",
      "\n",
      "1. Add a docstring to explain what the function does, what parameters it takes, and what it returns. This makes the code more readable and maintainable.\n",
      "\n",
      "2. Use more descriptive parameter names. While `width` and `length` are okay, `floor_width` and `floor_length` might be even clearer.\n",
      "\n",
      "3. Consider adding type hints to specify the expected types of the parameters and return value. This can help catch errors early.\n",
      "\n",
      "4. Check for invalid inputs. What if a negative number or zero is passed in? You might want to raise an exception.\n",
      "\n",
      "5. Consider returning a float instead of an int. Areas are often expressed as floating point numbers.\n",
      "\n",
      "Here's the updated function with these improvements:\n",
      "\n",
      "```python\n",
      "def calculate_rectangular_floor_area(floor_width: float, floor_length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        floor_width (float): The width of the floor.\n",
      "        floor_length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If either dimension is less than or equal to zero.\n",
      "    \"\"\"\n",
      "    if floor_width <= 0 or floor_length <= 0:\n",
      "        raise ValueError(\"Floor dimensions must be positive.\")\n",
      "\n",
      "    return floor_width * floor_length\n",
      "```\n",
      "\n",
      "These changes make the function more robust, readable, and maintainable. The docstring explains the function's purpose and usage, the type hints add clarity and error checking, the more descriptive names improve readability, and the input validation handles potential misuse.\n"
     ]
    }
   ],
   "source": [
    "print(system_1st_response[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e434256-8a1f-48bb-a74e-bac5478b7572",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_2nd_person_response = make_request(system_2nd_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c88e3c-a981-4124-b421-544b54fb2da4",
   "metadata": {},
   "source": [
    "print(system_2nd_person_response[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9156fa1f-c8ed-4bf5-a6e9-9b391b8cc4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_royal_we_response = make_request(system_royal_we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dfa5bac-7205-4a5a-9f9c-3065a742f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few ways to improve the given function:\n",
      "\n",
      "1. Add a docstring to explain what the function does and what parameters it takes:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width, length):\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "    \"\"\"\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "2. Add type hints to specify the expected types of the parameters and return value:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width: float, length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "    \"\"\"\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "3. Add input validation to ensure that the width and length are positive numbers:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width: float, length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the width or length is negative.\n",
      "    \"\"\"\n",
      "    if width <= 0 or length <= 0:\n",
      "        raise ValueError(\"Width and length must be positive.\")\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "These improvements make the function more readable, maintainable, and robust. The docstring provides clear documentation, the type hints make the expected types explicit, and the input validation ensures that the function only accepts valid input.\n"
     ]
    }
   ],
   "source": [
    "print(system_royal_we_response[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a5a60-28f9-4fd5-91fa-64efd33fb595",
   "metadata": {},
   "source": [
    "## Initial Observations\n",
    "We get different output from each prompt! I actually didn't expect this from such a small change. It's best to run them all through again to see if what we're observing is instability in the outputs or the differences are actually attributable to the difference between I/you/we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "904f3a0d-e048-4e08-9ba7-34787f450514",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_1st_response2 = make_request(system_1st_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7db099ae-58a2-476c-b4b0-bf047663dbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few ways to improve the given function:\n",
      "\n",
      "1. Add a docstring to explain what the function does and what parameters it takes:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width, length):\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "    \"\"\"\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "2. Add type hints to make the function more readable and maintainable:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width: float, length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "    \"\"\"\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "3. Add input validation to ensure that the width and length are positive numbers:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width: float, length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the width or length is negative.\n",
      "    \"\"\"\n",
      "    if width <= 0 or length <= 0:\n",
      "        raise ValueError(\"Width and length must be positive numbers.\")\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "4. Consider renaming the function to follow the Python naming convention of using underscores instead of camel case:\n",
      "\n",
      "```python\n",
      "def calculate_rectangular_floor_area(width: float, length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the width or length is negative.\n",
      "    \"\"\"\n",
      "    if width <= 0 or length <= 0:\n",
      "        raise ValueError(\"Width and length must be positive numbers.\")\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "These improvements make the function more readable, maintainable, and robust by providing clear documentation, type hints, input validation, and following the Python naming convention.\n"
     ]
    }
   ],
   "source": [
    "print(system_1st_response2[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "828b88fa-cbc3-4006-be54-e5d1a3b9dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_2nd_person_response2 = make_request(system_2nd_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68d381b3-75a7-44e5-97db-5073cc9a3595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few ways to improve the given function:\n",
      "\n",
      "1. Add a docstring to explain what the function does and what parameters it takes:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width, length):\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "    \"\"\"\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "2. Add type hints to make the function more readable and maintainable:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width: float, length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "    \"\"\"\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "3. Add input validation to ensure that the width and length are positive numbers:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width: float, length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the width or length is negative.\n",
      "    \"\"\"\n",
      "    if width <= 0 or length <= 0:\n",
      "        raise ValueError(\"Width and length must be positive numbers.\")\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "These improvements make the function more readable, maintainable, and robust. The docstring explains what the function does and what parameters it takes, making it easier for other developers to understand and use the function. The type hints make it clear what types of arguments the function expects and what type of value it returns. The input validation ensures that the function only accepts valid input and raises an error if the input is invalid.\n"
     ]
    }
   ],
   "source": [
    "print(system_2nd_person_response2[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81981b29-5e41-49d2-b20b-3555374dde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_royal_we_response2 = make_request(system_royal_we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83153e8f-3d0b-49ae-9b46-0afe3369aa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few ways to improve the given function:\n",
      "\n",
      "1. Add a docstring to explain what the function does and what parameters it takes:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width, length):\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "    \"\"\"\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "2. Add type hints to specify the expected types of the parameters and return value:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width: float, length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "    \"\"\"\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "3. Add input validation to ensure that the width and length are positive numbers:\n",
      "\n",
      "```python\n",
      "def calculate_area_rectangular_floor(width: float, length: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculates the area of a rectangular floor.\n",
      "\n",
      "    Args:\n",
      "        width (float): The width of the floor.\n",
      "        length (float): The length of the floor.\n",
      "\n",
      "    Returns:\n",
      "        float: The area of the floor.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the width or length is negative.\n",
      "    \"\"\"\n",
      "    if width <= 0 or length <= 0:\n",
      "        raise ValueError(\"Width and length must be positive.\")\n",
      "    return width * length\n",
      "```\n",
      "\n",
      "These improvements make the function more readable, maintainable, and robust. The docstring provides clear documentation, the type hints make the expected types explicit, and the input validation ensures that the function only accepts valid input.\n"
     ]
    }
   ],
   "source": [
    "print(system_royal_we_response2[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63507237-ec99-49e5-a338-03af7594362c",
   "metadata": {},
   "source": [
    "## Observations\n",
    "Ok so at the end of the day no major difference. It's interesting that the system seems to have converged on splitting out the suggestions, which is why it's important to re-run a prompt a few times. I personally prefer the first run where all of the advice is compiled into a single function. Let's see what it takes to return that style of output and make it consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa53c43e-533a-49db-b9df-afc5bb93d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "You are a software engineer with 20 years of experience in python and mentoring in coding.\n",
    "When presented with code snippets, you suggest how they can be improved by adding comments, \n",
    "improving performance, enhacing readibility, or whatever else you can think of.\n",
    "You explain your reasoning clearly and concisely.\n",
    "\n",
    "For example, when presented with the following prompt: \n",
    "<prompt>\n",
    "Given the following function:\n",
    "```python\n",
    "def calculate_area_rectangular_floor(width, length):\n",
    "  return width*length\n",
    "```\n",
    "How would you improve it?\n",
    "</prompt>\n",
    "\n",
    "Your must only include one rewrite of the code with all the suggestions, like this:\n",
    "<output>\n",
    "Here are a few ways to improve the given function:\n",
    "\n",
    "1. Add a docstring to explain what the function does, what parameters it takes, and what it returns. This makes the code more readable and maintainable.\n",
    "\n",
    "2. Use more descriptive parameter names. While `width` and `length` are okay, `floor_width` and `floor_length` might be even clearer.\n",
    "\n",
    "3. Consider adding type hints to specify the expected types of the parameters and return value. This can help catch errors early.\n",
    "\n",
    "4. Check for invalid inputs. What if a negative number or zero is passed in? You might want to raise an exception.\n",
    "\n",
    "5. Consider returning a float instead of an int. Areas are often expressed as floating point numbers.\n",
    "\n",
    "Here's the updated function with these improvements:\n",
    "\n",
    "```python\n",
    "def calculate_rectangular_floor_area(floor_width: float, floor_length: float) -> float:\n",
    "    \\\"\\\"\\\"\n",
    "    Calculates the area of a rectangular floor.\n",
    "\n",
    "    Args:\n",
    "        floor_width (float): The width of the floor.\n",
    "        floor_length (float): The length of the floor.\n",
    "\n",
    "    Returns:\n",
    "        float: The area of the floor.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If either dimension is less than or equal to zero.\n",
    "    \\\"\\\"\\\"\n",
    "    if floor_width <= 0 or floor_length <= 0:\n",
    "        raise ValueError(\"Floor dimensions must be positive.\")\n",
    "\n",
    "    return floor_width * floor_length\n",
    "```\n",
    "\n",
    "These changes make the function more robust, readable, and maintainable. The docstring explains the function's purpose and usage, the type hints add clarity and error checking, the more descriptive names improve readability, and the input validation handles potential misuse.\n",
    "</output>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d725716-01d5-4bb0-aa17-cdac7812344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_prompt = \"\"\"\n",
    "Given the following function:\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Prompt Improvement Tool')\n",
    "    parser.add_argument('prompt_file', type=str, help='The file containing the prompt to evaluate and improve')\n",
    "    parser.add_argument('--output', type=str, default='improved_prompt.txt', help='Output file for the improved prompt')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load the prompt from the specified file\n",
    "    with open(args.prompt_file, 'r') as f:\n",
    "        prompt = f.read()\n",
    "\n",
    "    print(\"Loaded prompt:\")\n",
    "    print(prompt)\n",
    "    print()\n",
    "    \n",
    "    type, correct, evaluation, suggestion = evaluate_prompt_type(prompt)\n",
    "    print(f\"Prompt type: {type}\")\n",
    "\n",
    "    if correct == \"Yes\":\n",
    "        print(\"Let's move on to scoring the prompt\")\n",
    "    else:\n",
    "        print(f\"Evaluation: \\n {evaluation}\")\n",
    "        print(f\"Here is a suggestion for how to improve the prompt: \\n {suggestion}\")\n",
    "\n",
    "    print(\"=========\")\n",
    "    scores = score_prompt(prompt)\n",
    "    print(scores)\n",
    "\n",
    "```\n",
    "How would you improve the code?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67ccbdba-7767-40f0-87e4-764ea9db30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_response = make_request(new_system_prompt, new_user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e3c0ca1-d7ed-470c-9c82-0966bfda4ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an improved version of the code with explanations:\n",
      "\n",
      "```python\n",
      "import argparse\n",
      "\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(description='Prompt Improvement Tool')\n",
      "    parser.add_argument('prompt_file', type=str, help='The file containing the prompt to evaluate and improve')\n",
      "    parser.add_argument('--output', type=str, default='improved_prompt.txt', help='Output file for the improved prompt')\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    # Load the prompt from the specified file\n",
      "    with open(args.prompt_file, 'r') as file:\n",
      "        prompt = file.read()\n",
      "\n",
      "    print(\"Loaded prompt:\")\n",
      "    print(prompt)\n",
      "    print()\n",
      "    \n",
      "    prompt_type, is_correct, evaluation, suggestion = evaluate_prompt_type(prompt)\n",
      "    print(f\"Prompt type: {prompt_type}\")\n",
      "\n",
      "    if is_correct:\n",
      "        print(\"Let's move on to scoring the prompt\")\n",
      "    else:\n",
      "        print(f\"Evaluation:\\n{evaluation}\")\n",
      "        print(f\"Here is a suggestion for how to improve the prompt:\\n{suggestion}\")\n",
      "\n",
      "    print(\"=========\")\n",
      "    scores = score_prompt(prompt)\n",
      "    print(scores)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "Improvements:\n",
      "\n",
      "1. Added `import argparse` at the top of the file. It's good practice to put all imports at the beginning of the file for clarity.\n",
      "\n",
      "2. Renamed the file handle variable from `f` to `file`. While `f` is a common convention, `file` is more descriptive and improves readability.\n",
      "\n",
      "3. Renamed the variables `type` and `correct` to `prompt_type` and `is_correct` respectively. `type` is a built-in function in Python, so it's best to avoid using it as a variable name. `is_correct` is a more descriptive boolean variable name than `correct`.\n",
      "\n",
      "4. Adjusted the formatting of the print statements for the evaluation and suggestion. Using `\\n` for line breaks within the f-strings improves readability.\n",
      "\n",
      "5. Added a `if __name__ == \"__main__\":` block to ensure that the `main()` function is only executed when the script is run directly, not when it's imported as a module. This is a best practice in Python.\n",
      "\n",
      "6. Added a blank line at the end of the file. It's a convention in Python to end files with a newline character.\n",
      "\n",
      "These changes improve the code's readability, maintainability, and adherence to Python best practices and conventions. The core functionality of the code remains the same.\n"
     ]
    }
   ],
   "source": [
    "print(new_response[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7774426a-5d42-4009-abfd-31284ed323b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run a few times to get an idea of the stability of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0707ed3-535b-4af5-97b1-b3b81a6ec05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_responses = [make_request(new_system_prompt, new_user_prompt) for _ in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "874ed701-7a99-46ef-95f4-aa8ceeeaad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an improved version of the code with explanations:\n",
      "\n",
      "```python\n",
      "import argparse\n",
      "\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(description='Prompt Improvement Tool')\n",
      "    parser.add_argument('prompt_file', type=str, help='The file containing the prompt to evaluate and improve')\n",
      "    parser.add_argument('--output', type=str, default='improved_prompt.txt', help='Output file for the improved prompt')\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    # Load the prompt from the specified file\n",
      "    with open(args.prompt_file, 'r') as file:\n",
      "        prompt = file.read()\n",
      "\n",
      "    print(\"Loaded prompt:\")\n",
      "    print(prompt)\n",
      "    print()\n",
      "    \n",
      "    prompt_type, is_correct, evaluation, suggestion = evaluate_prompt_type(prompt)\n",
      "    print(f\"Prompt type: {prompt_type}\")\n",
      "\n",
      "    if is_correct:\n",
      "        print(\"Let's move on to scoring the prompt\")\n",
      "    else:\n",
      "        print(f\"Evaluation:\\n{evaluation}\")\n",
      "        print(f\"Here is a suggestion for how to improve the prompt:\\n{suggestion}\")\n",
      "\n",
      "    print(\"=========\")\n",
      "    scores = score_prompt(prompt)\n",
      "    print(scores)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "Improvements:\n",
      "\n",
      "1. Added `import argparse` at the top of the file. It's good practice to put all imports at the beginning of the file for clarity.\n",
      "\n",
      "2. Renamed the file handle variable from `f` to `file`. While `f` is a common convention, `file` is more descriptive and improves readability.\n",
      "\n",
      "3. Renamed the variables `type` and `correct` to `prompt_type` and `is_correct` respectively. `type` is a built-in function in Python, so it's best to avoid using it as a variable name. `is_correct` is a more descriptive boolean variable name than `correct`.\n",
      "\n",
      "4. Adjusted the formatting of the print statements for the evaluation and suggestion. Using `\\n` for line breaks within the f-strings improves readability.\n",
      "\n",
      "5. Added a `if __name__ == \"__main__\":` block to ensure that the `main()` function is only executed when the script is run directly, not when it's imported as a module. This is a best practice in Python.\n",
      "\n",
      "6. Added a blank line at the end of the file. It's a convention in Python to end files with a newline character.\n",
      "\n",
      "These changes improve the code's readability, maintainability, and adherence to Python best practices and conventions. The core functionality of the code remains the same.\n",
      "\n",
      "==========\n",
      "\n",
      "Here's an improved version of the code with explanations:\n",
      "\n",
      "```python\n",
      "import argparse\n",
      "\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(description='Prompt Improvement Tool')\n",
      "    parser.add_argument('prompt_file', type=str, help='The file containing the prompt to evaluate and improve')\n",
      "    parser.add_argument('--output', type=str, default='improved_prompt.txt', help='Output file for the improved prompt')\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    # Load the prompt from the specified file\n",
      "    with open(args.prompt_file, 'r') as file:\n",
      "        prompt = file.read()\n",
      "\n",
      "    print(\"Loaded prompt:\")\n",
      "    print(prompt)\n",
      "    print()\n",
      "    \n",
      "    prompt_type, is_correct, evaluation, suggestion = evaluate_prompt_type(prompt)\n",
      "    print(f\"Prompt type: {prompt_type}\")\n",
      "\n",
      "    if is_correct:\n",
      "        print(\"Let's move on to scoring the prompt\")\n",
      "    else:\n",
      "        print(f\"Evaluation:\\n{evaluation}\")\n",
      "        print(f\"Here is a suggestion for how to improve the prompt:\\n{suggestion}\")\n",
      "\n",
      "    print(\"=========\")\n",
      "    scores = score_prompt(prompt)\n",
      "    print(scores)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "Improvements:\n",
      "\n",
      "1. Added `import argparse` at the top of the file. It's good practice to put all imports at the beginning of the file for clarity.\n",
      "\n",
      "2. Renamed the file handle variable from `f` to `file`. While `f` is a common convention, `file` is more descriptive and improves readability.\n",
      "\n",
      "3. Renamed the variables `type` and `correct` to `prompt_type` and `is_correct` respectively. `type` is a built-in function in Python, so it's best to avoid using it as a variable name. `is_correct` is a more descriptive boolean variable name than `correct`.\n",
      "\n",
      "4. Adjusted the formatting of the print statements for the evaluation and suggestion. Using `\\n` for line breaks within the f-strings improves readability.\n",
      "\n",
      "5. Added a `if __name__ == \"__main__\":` block to ensure that the `main()` function is only executed when the script is run directly, not when it's imported as a module. This is a best practice in Python.\n",
      "\n",
      "6. Added a blank line at the end of the file. It's a convention in Python to end files with a newline character.\n",
      "\n",
      "These changes improve the code's readability, maintainability, and adherence to Python best practices and conventions. The core functionality of the code remains the same.\n",
      "\n",
      "==========\n",
      "\n",
      "Here's an improved version of the code with explanations:\n",
      "\n",
      "```python\n",
      "import argparse\n",
      "\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(description='Prompt Improvement Tool')\n",
      "    parser.add_argument('prompt_file', type=str, help='The file containing the prompt to evaluate and improve')\n",
      "    parser.add_argument('--output', type=str, default='improved_prompt.txt', help='Output file for the improved prompt')\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    # Load the prompt from the specified file\n",
      "    with open(args.prompt_file, 'r') as file:\n",
      "        prompt = file.read()\n",
      "\n",
      "    print(\"Loaded prompt:\")\n",
      "    print(prompt)\n",
      "    print()\n",
      "    \n",
      "    prompt_type, is_correct, evaluation, suggestion = evaluate_prompt_type(prompt)\n",
      "    print(f\"Prompt type: {prompt_type}\")\n",
      "\n",
      "    if is_correct:\n",
      "        print(\"Let's move on to scoring the prompt\")\n",
      "    else:\n",
      "        print(f\"Evaluation:\\n{evaluation}\")\n",
      "        print(f\"Here is a suggestion for how to improve the prompt:\\n{suggestion}\")\n",
      "\n",
      "    print(\"=========\")\n",
      "    scores = score_prompt(prompt)\n",
      "    print(scores)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "Improvements and explanations:\n",
      "\n",
      "1. Added `import argparse` at the top of the file. It's good practice to put all imports at the beginning of the file for clarity.\n",
      "\n",
      "2. Changed the variable name `f` to `file` in the `with` statement. Using more descriptive variable names improves code readability.\n",
      "\n",
      "3. Renamed the variables `type` and `correct` to `prompt_type` and `is_correct`, respectively. `type` is a built-in function in Python, so it's best to avoid using it as a variable name. `is_correct` is a more descriptive name than `correct`.\n",
      "\n",
      "4. Adjusted the formatting of the print statements for the evaluation and suggestion. Using `\\n` for line breaks within the f-strings improves readability.\n",
      "\n",
      "5. Added a check for `__name__ == \"__main__\"` to ensure that the `main()` function is only executed when the script is run directly, not when it's imported as a module. This is a common Python convention.\n",
      "\n",
      "6. Assumed the existence of `evaluate_prompt_type()` and `score_prompt()` functions. Make sure these functions are defined correctly in the same file or imported from another module.\n",
      "\n",
      "7. Consider adding error handling for file I/O operations and function calls. You can use `try`/`except` blocks to catch and handle potential exceptions gracefully.\n",
      "\n",
      "8. If the `improved_prompt.txt` file is meant to store the improved prompt, consider writing the improved prompt to the file within the script.\n",
      "\n",
      "9. Add comments and docstrings to explain the purpose and functionality of the script and its functions. This enhances code maintainability and makes it easier for other developers to understand.\n",
      "\n",
      "These improvements focus on code readability, maintainability, and following Python best practices and conventions. The actual functionality of the script remains the same.\n",
      "\n",
      "==========\n",
      "\n",
      "Here's an improved version of the code with explanations:\n",
      "\n",
      "```python\n",
      "import argparse\n",
      "\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(description='Prompt Improvement Tool')\n",
      "    parser.add_argument('prompt_file', type=str, help='The file containing the prompt to evaluate and improve')\n",
      "    parser.add_argument('--output', type=str, default='improved_prompt.txt', help='Output file for the improved prompt')\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    # Load the prompt from the specified file\n",
      "    with open(args.prompt_file, 'r') as file:\n",
      "        prompt = file.read()\n",
      "\n",
      "    print(\"Loaded prompt:\")\n",
      "    print(prompt)\n",
      "    print()\n",
      "    \n",
      "    prompt_type, is_correct, evaluation, suggestion = evaluate_prompt_type(prompt)\n",
      "    print(f\"Prompt type: {prompt_type}\")\n",
      "\n",
      "    if is_correct:\n",
      "        print(\"Let's move on to scoring the prompt\")\n",
      "    else:\n",
      "        print(f\"Evaluation:\\n{evaluation}\")\n",
      "        print(f\"Here is a suggestion for how to improve the prompt:\\n{suggestion}\")\n",
      "\n",
      "    print(\"=========\")\n",
      "    scores = score_prompt(prompt)\n",
      "    print(scores)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "Improvements and explanations:\n",
      "\n",
      "1. Added `import argparse` at the top of the file. It's good practice to put all imports at the beginning of the file for clarity.\n",
      "\n",
      "2. Changed the variable name `f` to `file` in the `with` statement. Using more descriptive variable names improves code readability.\n",
      "\n",
      "3. Renamed the variables `type` and `correct` to `prompt_type` and `is_correct`, respectively. `type` is a built-in function in Python, so it's best to avoid using it as a variable name. `is_correct` is a more descriptive name than `correct`.\n",
      "\n",
      "4. Adjusted the formatting of the print statements for the evaluation and suggestion. Using `\\n` for line breaks within the f-strings improves readability.\n",
      "\n",
      "5. Added a check for `__name__ == \"__main__\"` to ensure that the `main()` function is only executed when the script is run directly, not when it's imported as a module. This is a common Python convention.\n",
      "\n",
      "6. Assumed the existence of `evaluate_prompt_type()` and `score_prompt()` functions. Make sure these functions are defined correctly in the same file or imported from another module.\n",
      "\n",
      "7. Consider adding error handling for file I/O operations and function calls. You can use `try`/`except` blocks to catch and handle potential exceptions gracefully.\n",
      "\n",
      "8. If the `improved_prompt.txt` file is meant to store the improved prompt, consider writing the improved prompt to the file within the script.\n",
      "\n",
      "9. Add comments and docstrings to explain the purpose and functionality of the script and its functions. This enhances code maintainability and makes it easier for other developers to understand.\n",
      "\n",
      "These improvements focus on code readability, maintainability, and following Python best practices and conventions. The actual functionality of the script remains the same.\n",
      "\n",
      "==========\n",
      "\n",
      "Here's an improved version of the code with explanations:\n",
      "\n",
      "```python\n",
      "import argparse\n",
      "\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(description='Prompt Improvement Tool')\n",
      "    parser.add_argument('prompt_file', type=str, help='The file containing the prompt to evaluate and improve')\n",
      "    parser.add_argument('--output', type=str, default='improved_prompt.txt', help='Output file for the improved prompt')\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    # Load the prompt from the specified file\n",
      "    with open(args.prompt_file, 'r') as file:\n",
      "        prompt = file.read()\n",
      "\n",
      "    print(\"Loaded prompt:\")\n",
      "    print(prompt)\n",
      "    print()\n",
      "    \n",
      "    prompt_type, is_correct, evaluation, suggestion = evaluate_prompt_type(prompt)\n",
      "    print(f\"Prompt type: {prompt_type}\")\n",
      "\n",
      "    if is_correct:\n",
      "        print(\"Let's move on to scoring the prompt\")\n",
      "    else:\n",
      "        print(f\"Evaluation:\\n{evaluation}\")\n",
      "        print(f\"Here is a suggestion for how to improve the prompt:\\n{suggestion}\")\n",
      "\n",
      "    print(\"=========\")\n",
      "    scores = score_prompt(prompt)\n",
      "    print(scores)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "Improvements:\n",
      "\n",
      "1. Added `import argparse` at the top of the file. It's good practice to put all imports at the beginning of the file for clarity.\n",
      "\n",
      "2. Renamed the file handle variable from `f` to `file`. While `f` is a common convention, `file` is more descriptive and improves readability.\n",
      "\n",
      "3. Renamed the variables `type` and `correct` to `prompt_type` and `is_correct` respectively. `type` is a built-in function in Python, so it's best to avoid using it as a variable name. `is_correct` is a more descriptive boolean variable name than `correct`.\n",
      "\n",
      "4. Adjusted the formatting of the print statements for the evaluation and suggestion. Using `\\n` for line breaks within the f-strings improves readability.\n",
      "\n",
      "5. Added a `if __name__ == \"__main__\":` block to ensure that the `main()` function is only executed when the script is run directly, not when it's imported as a module. This is a best practice in Python.\n",
      "\n",
      "6. Added a blank line at the end of the file. It's a convention in Python to end files with a newline character.\n",
      "\n",
      "These changes improve the code's readability, maintainability, and adherence to Python best practices and conventions. The core functionality of the code remains the same.\n"
     ]
    }
   ],
   "source": [
    "reports = \"\\n\\n==========\\n\\n\".join([response[0].text for response in new_responses])\n",
    "print(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b84d55-04e2-4649-a6e9-4d9e8ee68422",
   "metadata": {},
   "source": [
    "## Observations\n",
    "The prompt is now more subjectively stable. Some versions include 9 tips and the others 6. There's a lot here that would be easy to miss - so let's see if we can get claude to report on what's different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fe91a33-4e0f-4859-9e9a-93cb936cdca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_system_prompt = \"\"\"\n",
    "You are an expert software engineer that has a keen eye for documentation.\n",
    "You detect meaningful diferences in the outputs of prompts from LLMs and provide succinct summaries\n",
    "of those commonalities in differences.\n",
    "\"\"\"\n",
    "reporting_prompt = f\"\"\"\n",
    "Given the following 5 responses from an LLM, provide an overview of their similarities and differences. You can reference each prompt response\n",
    "by its order. They will be separated with multiple equal signs, ie: \"==========\"\n",
    "\n",
    "Responses:\n",
    "{reports}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d53f831-cace-40e4-97ed-cb450ac047b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporting_response = make_request(reporting_system_prompt, reporting_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea2453c6-e35e-484e-ad27-a8fe90abc6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided responses are very similar, with only minor differences in wording and formatting. Here's an overview of their similarities and differences:\n",
      "\n",
      "Similarities:\n",
      "1. All responses provide an improved version of the code with explanations.\n",
      "2. They include the same code snippet with identical improvements.\n",
      "3. The explanations cover similar points, such as renaming variables, adjusting print statement formatting, and adding a `if __name__ == \"__main__\":` block.\n",
      "4. They assume the existence of `evaluate_prompt_type()` and `score_prompt()` functions.\n",
      "\n",
      "Differences:\n",
      "1. Response 3 provides more detailed explanations for each improvement point compared to the other responses.\n",
      "2. Response 3 suggests adding error handling for file I/O operations and function calls using `try`/`except` blocks, which is not mentioned in the other responses.\n",
      "3. Response 3 also recommends writing the improved prompt to the `improved_prompt.txt` file if that's the intended purpose, which is not explicitly mentioned in the other responses.\n",
      "4. Response 3 suggests adding comments and docstrings to enhance code maintainability and readability, which is not mentioned in the other responses.\n",
      "5. Response 5 mentions adding a blank line at the end of the file as a Python convention, which is not explicitly stated in the other responses.\n",
      "\n",
      "Overall, the responses provide very similar improvements and explanations for the given code snippet. The main differences lie in the level of detail in the explanations and a few additional suggestions mentioned in some responses but not others.\n"
     ]
    }
   ],
   "source": [
    "print(reporting_response[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f1091-567a-4445-9bf5-65ce4bb85958",
   "metadata": {},
   "source": [
    "## Final Observation\n",
    "The difference between prompting Claude with I/you/we was negligible and had little predictable impact on responses in this small\n",
    "test set. The important of iteration and refining prompts - testing the same prompt multiple times to ensure \"stability\" is more \n",
    "important. Using Claude as a way to subjectively assess the output is a clever way to ensure impartiality and consistency in the\n",
    "notion of \"stability\".\n",
    "\n",
    "Speaking of, let's re-run the stablity check to see if it itself is \"stable\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5bb60846-aa71-4d65-bbc4-33b0ef93207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided responses are very similar, with only minor differences. Here's an overview of their similarities and differences:\n",
      "\n",
      "Similarities:\n",
      "1. All responses provide an improved version of the code with explanations.\n",
      "2. They all include the same code snippet with the same improvements.\n",
      "3. The explanations for the improvements are mostly the same, covering topics such as:\n",
      "   - Adding `import argparse` at the top of the file\n",
      "   - Renaming variables for better readability and avoiding built-in function names\n",
      "   - Adjusting the formatting of print statements\n",
      "   - Adding a `if __name__ == \"__main__\":` block\n",
      "   - Assuming the existence of `evaluate_prompt_type()` and `score_prompt()` functions\n",
      "\n",
      "Differences:\n",
      "1. Response 3 provides more detailed explanations for each improvement, while the other responses have shorter explanations.\n",
      "2. Response 3 includes additional suggestions, such as adding error handling, writing the improved prompt to a file, and adding comments and docstrings.\n",
      "3. Response 5 mentions adding a blank line at the end of the file as a Python convention, which is not mentioned in the other responses.\n",
      "\n",
      "Overall, the responses are highly consistent, providing the same improved code and similar explanations for the improvements. The main differences lie in the level of detail in the explanations and a few additional suggestions in some responses.\n"
     ]
    }
   ],
   "source": [
    "reporting_response2 = make_request(reporting_system_prompt, reporting_prompt)\n",
    "print(reporting_response2[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52298296-e826-45ad-a581-6a9f28e5e67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided responses are very similar, with only minor differences in wording and formatting. Here's an overview of their similarities and differences:\n",
      "\n",
      "Similarities:\n",
      "1. All responses provide an improved version of the code with explanations.\n",
      "2. They include the same code snippet with identical improvements.\n",
      "3. The explanations cover similar points, such as renaming variables, adjusting print statement formatting, and adding a `if __name__ == \"__main__\":` block.\n",
      "4. They assume the existence of `evaluate_prompt_type()` and `score_prompt()` functions.\n",
      "\n",
      "Differences:\n",
      "1. Response 3 provides more detailed explanations for each improvement point compared to the other responses.\n",
      "2. Response 3 suggests adding error handling for file I/O operations and function calls using `try`/`except` blocks, which is not mentioned in the other responses.\n",
      "3. Response 3 also recommends writing the improved prompt to the `improved_prompt.txt` file if that's the intended purpose, which is not explicitly mentioned in the other responses.\n",
      "4. Response 3 suggests adding comments and docstrings to enhance code maintainability and readability, which is not mentioned in the other responses.\n",
      "5. Response 5 mentions adding a blank line at the end of the file as a Python convention, which is not explicitly stated in the other responses.\n",
      "\n",
      "Overall, the responses provide very similar improvements and explanations for the given code snippet. The main differences lie in the level of detail in the explanations and a few additional suggestions mentioned in some responses but not others.\n"
     ]
    }
   ],
   "source": [
    "reporting_response3 = make_request(reporting_system_prompt, reporting_prompt)\n",
    "print(reporting_response3[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc3a78-2800-4c4f-805a-c61e179763af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anthropic-tools",
   "language": "python",
   "name": "anthropic-tools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
